Nel machine learning tradizionale, quando si lavora con dataset ad alta dimensionalità, è spesso necessario ridurre il numero di feature mantenendo solo quelle realmente informative. Alcune euristiche topologiche o attributi dei nodi possono infatti introdurre **informazione ridondante o irrilevante**: per esempio, una feature quasi costante o con oscillazioni minime attorno al valore più basso non contribuisce all’apprendimento e può essere eliminata senza perdita di qualità. Al contrario, una feature con **grande varianza** porta molta informazione, perché permette di distinguere meglio gli esempi e influisce significativamente sulla capacità del modello di imparare.
## **PCA (Principal Component Analysis)**

> [!important] Ricorda  
> **PCA sfrutta esattamente questo principio:** identifica le direzioni nei dati con la varianza più elevata e utilizza queste componenti per rappresentare l’informazione essenziale, riducendo la dimensionalità senza compromettere i pattern principali.

E' una tecnica **lineare** per ridurre la dimensionalità e visualizzare i dati.  
Dinamica:
- identifica le direzioni nei dati con la massima varianza;
- ordina queste direzioni (componenti principali) in base alla quantità di informazione che spiegano;
- proietta i dati sulle prime _k_ componenti principali, ottenendo una rappresentazione compatta con perdita minima di informazione.

> [!tip] Ricorda  
> PCA è molto utile per visualizzare **embedding ad alta dimensionalità** in 2D o 3D, facilitando l’analisi della struttura del grafo.
### **PCA: Formulazione Matematica**

Dato un dataset rappresentato da una matrice:

$$
X \in \mathbb{R}^{n \times D}
$$

dove ogni riga è un **sample** e ogni colonna è una **feature**, la PCA identifica le direzioni più informative del dataset e proietta i dati in uno spazio di dimensione ridotta.
#### **1. Centering dei dati**
Si rimuove la media da ogni feature per centrare il dataset nell'origine:
$$
X_{\text{centered}} = X - \mu
$$
dove $\mu$ è il vettore delle medie delle colonne di $X$.
#### **2. Covariance Matrix**
Si calcola la matrice di covarianza (simmetrica):
$$
S = \frac{1}{n - 1} X_{\text{centered}}^\top X_{\text{centered}}
$$
Questa matrice cattura come le feature variano insieme (correlazione).
#### **3. Autovettori e Autovalori**
Si calcolano gli autovettori della matrice di covarianza:
$$
w_1, w_2, \ldots, w_D
$$
Questi vengono ordinati in base ai corrispondenti **autovalori** in ordine decrescente.
* Autovalore alto $\to$ molta varianza (molta informazione).
* Autovalore basso $\to$ rumore o ridondanza.
#### **4. Proiezione sui primi k componenti**
Si selezionano i primi $k$ autovettori per costruire la matrice di proiezione $W_k$:
$$
W_k = [ w_1, w_2, \dots, w_k ]
$$
La proiezione dei dati nello spazio ridotto $Z$ si ottiene moltiplicando la matrice centrata per la matrice di proiezione:
$$
Z = X_{\text{centered}} \cdot W_k
$$
#### **Risultato finale**
Ogni sample originario di dimensione $D$ viene trasformato in un vettore di dimensione:
$$
k \ll D
$$
preservando la massima varianza possibile dei dati originali.

> [!important] **Intuizione**
>
> Gli autovettori identificati dalla PCA rappresentano le **direzioni principali di variazione** del dataset.
> - Se una direzione ha **bassa varianza**, i dati sono "piatti" lungo quell'asse $\to$ poca informazione, può essere compressa.
> - Le componenti con **alta varianza** (PC1, PC2...) sono quelle che meglio distinguono i punti nello spazio.
>
> La PCA ruota gli assi per allinearli a queste direzioni e scarta quelli meno significativi.

Lavorando con dati complessi e molto estesi, come embedding dei nodi, lunghi vettori di feature o dataset con decine di attributi, diventa difficile visualizzarli o interpretarli direttamente. Le tecniche di **dimensionality reduction** risolvono questo problema proiettando i dati in uno spazio a dimensione inferiore, preservando comunque le strutture fondamentali come cluster, separazioni tra classi e outlier.
## **t-SNE**

t-SNE (**t-distributed stochastic neighbor embedding**) è un algoritmo di **dimensionality reduction non lineare** pensato per visualizzare dati complessi in **2D/3D**.

- Cluster naturali
- Pattern nascosti
- Outlier

> [!note]  Nota
> A differenza di PCA, t-SNE non mira alla conservazione della varianza, ma alla preservazione delle **relazioni locali** tra i punti. Per questo è molto usato per visualizzare strutture complesse come gli embedding neurali.

- Modella le **similarità** tra punti nello spazio ad alta dimensionalità.

-  Trova un embedding a bassa dimensionalità che **preserva le relazioni locali**.

- **Vicini in high-dim → vicini in low-dim.**

## Differenze chiave rispetto a PCA

- PCA → **lineare**, preserva **varianza globale**.

- t-SNE → **non lineare**, preserva **solo la struttura locale**, non le distanze globali.

- PCA mantiene forme geometriche su ampia scala, t-SNE evidenzia cluster locali.

>[!caution] Distanza tra punti ad alta dimensionalità
>Dato: 
>- Punto **A = (xₐ₁, xₐ₂, ..., xₐₙ)**
>- Punto **B = (x_b₁, x_b₂, ..., x_bₙ)**
>- Con n = numero di feature
La distanza euclidea sarà: 
$$d_{AB} = \sqrt{ \sum_{i=1}^{n} (x_{a_i} - x_{b_i})^2 }$$
