Nel machine learning tradizionale, quando si lavora con dataset ad alta dimensionalità, è spesso necessario ridurre il numero di feature mantenendo solo quelle realmente informative. Alcune euristiche topologiche o attributi dei nodi possono infatti introdurre **informazione ridondante o irrilevante**: per esempio, una feature quasi costante o con oscillazioni minime attorno al valore più basso non contribuisce all’apprendimento e può essere eliminata senza perdita di qualità. Al contrario, una feature con **grande varianza** porta molta informazione, perché permette di distinguere meglio gli esempi e influisce significativamente sulla capacità del modello di imparare.
### **PCA (Principal Component Analysis)**

> [!important] Ricorda  
> **PCA sfrutta esattamente questo principio:** identifica le direzioni nei dati con la varianza più elevata e utilizza queste componenti per rappresentare l’informazione essenziale, riducendo la dimensionalità senza compromettere i pattern principali.

E' una tecnica **lineare** per ridurre la dimensionalità e visualizzare i dati.  
Dinamica:
- identifica le direzioni nei dati con la massima varianza;
- ordina queste direzioni (componenti principali) in base alla quantità di informazione che spiegano;
- proietta i dati sulle prime _k_ componenti principali, ottenendo una rappresentazione compatta con perdita minima di informazione.

> [!tip] Ricorda  
> PCA è molto utile per visualizzare **embedding ad alta dimensionalità** in 2D o 3D, facilitando l’analisi della struttura del grafo.

Lavorando con dati complessi e molto estesi, come embedding dei nodi, lunghi vettori di feature o dataset con decine di attributi, diventa difficile visualizzarli o interpretarli direttamente. Le tecniche di **dimensionality reduction** risolvono questo problema proiettando i dati in uno spazio a dimensione inferiore, preservando comunque le strutture fondamentali come cluster, separazioni tra classi e outlier.
### **t-SNE**

t-SNE è una tecnica **non lineare**, pensata soprattutto per la visualizzazione di: 

- Cluster naturali
- Pattern nascosti
- Outlier

> [!note]  Nota
> A differenza di PCA, t-SNE non mira alla conservazione della varianza, ma alla preservazione delle **relazioni locali** tra i punti. Per questo è molto usato per visualizzare strutture complesse come gli embedding neurali.

