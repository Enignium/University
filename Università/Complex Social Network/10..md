# Local Heuristics

Sono le regole che un nodo può applicare usando solo le informazioni locali, cioè tutto ciò che può osservare nel proprio intorno immediato nella rete, senza conoscere l'intera struttura globale.

Di base:

- Ogni nodo può basarsi solo sui dati facilmente accessibili
- Prendere decisioni rapide e "a basso costo"
- ottenere comunque risultati utili per problemi come: diffusione, ricerca di link, navigazione nella rete, scelta di nodi importanti

Leggi:

- Degree heuristic: preferire nodi con grado alto tra i vicini
- Greedy heuristic: scegliere il vicino massimizza/minimizza una certa proprietà (distanza stimata tipo)
- Local similarity heuristic: misurare quanto due nodi sono simili guardando solo i loro vini (neighbors, Jaccard, Adamic-Adar, ecc.)
- Local centrality heuristic: stimare l'importanza di un nodo usando solo la struttura locale (es. quanto è connesso nel suo vicinato)

Vantaggi: Sono scalabili, hanno un basso costo computazionale e un'adattabilità dinamica, quindi funzionano in reti enormi, non serve calcolare metriche globali, se la rete cambia, le decisioni locali si aggiornano subito senza calcolare nulla di globale
## Local Similarity Heuristic:
### Common neighbors

È la misura **più semplice** e più “grezza” tra le local heuristics per la _link prediction_. Conta **quanti vicini in comune** hanno due nodi non ancora collegati. $$
CN(u, v) = |N(u) \cap N(v)|
$$
- Più due nodi hanno amici/vicini in comune → più è probabile che si colleghino.
- Nessuna normalizzazione, nessun peso.
- Usa solo l’informazione minima: l’intersezione dei vicini.
### Jaccard Coefficient (JC)

Misura **quanto due nodi sono simili** guardando l’intersezione dei loro vicini. $$
J(u, v) = \frac{|N(u) \cap N(v)|}{|N(u) \cup N(v)|}
$$
- Più vicini in comune $\to$ valore vicino a 1
- pochi o nessun vicino in comune $\to$ valore vicino a 0
- usa solo informazione locale (i vicini immediati)
### Adamic–Adar (AA)

È una variante “raffinata” dei vicini comuni:  
non conta solo _quanti_ vicini in comune hanno due nodi, ma anche **quanto sono rari** questi vicini. $$
AA(u, v) = \sum_{w \in N(u) \cap N(v)} \frac{1}{\log k_w}
$$
- Se un vicino comune w ha alto grado, quindi conosce tutti, il suo contributo è piccolo
- Se w ha basso grado, connessioni rare quindi, il suo contributo pesa di più

Questo indice funziona meglio quando la rete ha hub grandi che rendono ingannevole il solo conteggio dei vicini comuni.
### Preferential Attachment (PA)

Qui non si guarda ai vicini in comune.  
L’idea è diversa: **due nodi con grado alto hanno naturalmente più probabilità di collegarsi**. $$
PA(u, v) = k_u \cdot k_v
$$
- Produce un valore alto quando **entrambi** i nodi hanno grado elevato;
- Riflette la logica dei modelli scale-free: _“the rich get richer”_;
- Non serve alcuna informazione sull’intersezione dei vicini, solo i due gradi.

È molto usato per reti in cui la crescita si basa sul meccanismo di attacco preferenziale (come Barabási–Albert).
### Resource Allocation (RA)

E' molto simile all’Adamic–Adar, ma invece di usare il log del grado, usa **direttamente il grado** dei vicini comuni. $$
RA(u, v) = \sum_{w \in N(u) \cap N(v)} \frac{1}{k_w}
$$
- Immagina che ogni vicino comune w abbia **1 unità di risorsa** da distribuire ai suoi vicini
- Un nodo con grado **alto** divide la sua risorsa tra molti → contribuisce poco
- Un nodo con grado **basso** divide tra pochi → contribuisce molto

È ancora più “duro” dell’Adamic–Adar nel **penalizzare gli hub**, e spesso funziona bene nelle reti sparse o con molti nodi a basso grado.
#### Valutazione dei diversi modelli:

- Common neighbors: Semplice e veloce, non distingue però tra vicini rari e comuni
- Jaccard: corregge l'effetto dei nodi alto grado, può sottostimare nodi ad altissimo grado anche se rilevanti
- Adamic-Adar: Riduce l'impatto degli hub; vicino comune raro $\to$ contributo forte, ancora sensibile in presenza di super hub
- Resource Allocation: Penalizza gli hub ancora di più $\to$molto selettivo, ma può essere troppo aggressivo in reti non sparse
- Preferential Attachment: Molto veloce, indipendentemente dai vicini comuni, ma spesso totalmente fuorviante in reti reali, favorisce solo gli hub
## Global Similarity Measure
### Katz Index

E' una misura di **similarità globale**, non più puramente locale come CN, Jaccard, AA, RA o PA. $$
Katz(u, v) = \sum_{l=1}^{\infty} \beta^l \cdot |\text{walks}_{u,v}^{(l)}|
$$dove:

- l è la lunghezza del cammino
- $|\text{walks}_{u,v}^{(l)}|$ è il numero di cammini di lunghezza l tra u e v
- $\beta$ è un fattore di attenuazione, riduce l'importanza dei cammini

Quindi:

- Percorsi corti $\to$ molto importanti
- Percorsi lunghi $\to$ contano ma meno
- Se due nodi sono immersi nello stesso “intorno strutturale” della rete, avranno punteggio altissimo

Due nodi sono più simili se esistono percorsi che li collegano. Katz non usa solo i vicini di primo grado o il grado dei nodi: usa **l’intera struttura del grafo** → è una misura globale.

### Rooted PageRank (o Personalized PageRank)

Quando si analizza il comportamento di un random walker su una rete, non esiste realmente un “punto di inizio” che determini l’esito finale del processo. All’inizio puoi immaginare il walker che parte da un nodo qualunque e sceglie in modo casuale uno dei suoi vicini per muoversi, usando ogni volta un numero randomico per decidere la direzione. Se ripeti questa operazione per molti passi, il walker continua a esplorare la rete senza memoria: non importa da dove è partito, dopo un numero sufficientemente grande di passi il modo in cui visita i nodi tende sempre alla stessa distribuzione stabile. Questa distribuzione è chiamata **distribuzione stazionaria** del random walk.

La cosa importante è capire cosa determina la probabilità di trovarsi in ciascun nodo nel lungo periodo. In un random walk uniforme, ogni volta che il walker è su un nodo con grado (k), sceglie uno dei suoi (k) vicini con probabilità (1/k). Questo semplice meccanismo genera un risultato fondamentale: la probabilità di trovarsi in un nodo (v), quando il tempo tende all’infinito, risulta **proporzionale al suo grado**. Gli hub hanno più modi per essere raggiunti, come se avessero più porte d’ingresso, e quindi il walker finisce su di loro più frequentemente. Al contrario, un nodo isolato o con pochi collegamenti ha poche vie di accesso, dunque la probabilità di trovarlo è molto più bassa. Tutto questo accade spontaneamente, senza alcun tipo di bias.

Il **Rooted PageRank** modifica questo comportamento introducendo una preferenza esplicita per un nodo specifico, chiamato radice. L’idea è che il walker continui a muoversi come sempre, scegliendo un vicino casuale, ma a intervalli regolari, con probabilità $1-\alpha$ e ritorni immediatamente al nodo radice (r). In questo modo, la distribuzione finale non rappresenta più l’importanza globale di ogni nodo nella rete, ma misura quanto esso è “vicino”, in termini di cammini possibili, proprio al nodo radice. In altre parole, la domanda non è più “quali nodi vengono visitati più spesso da un walker casuale?”, ma “quali nodi vengono visitati più spesso da un walker che parte sempre da (r) e continua a tornare su (r) mentre esplora la rete?”.

L’effetto di questo cambiamento è molto significativo. Nel random walk standard gli hub dominano naturalmente la distribuzione, perché il walker ci finisce continuamente. Nel rooted PageRank, invece, anche se gli hub restano più popolati del resto della rete, entrano in gioco soprattutto i cammini che partono dal nodo radice: se un nodo è raggiungibile da molti percorsi, soprattutto se brevi, riceverà un punteggio alto. Se invece è lontano, raggiungibile solo tramite cammini lunghissimi o poco probabili, il suo punteggio diminuirà drasticamente, indipendentemente da quanto è grande il suo grado. In questo senso il rooted PageRank è una misura che combina **informazione globale** e **prospettiva locale** del nodo sorgente.

In sintesi, il random walk standard spiega perché alcuni nodi (gli hub) risultano naturalmente più “centrali”: la probabilità stazionaria di trovarsi in un nodo è direttamente proporzionale al suo grado. Il rooted PageRank, invece, rompe questa simmetria introducendo un punto di vista preferito: un walker che torna ripetutamente al nodo sorgente permette di misurare quanto ciascun nodo è strutturalmente vicino a quel punto, scontando automaticamente i cammini lunghi e privilegiando quelli più brevi e più numerosi.

### SimRank

Due nodi sono simili se sono collegati a nodi simili.

È un’idea **ricorsiva**, molto diversa da tutte le heuristics locali viste finora (CN, Jaccard, Adamic–Adar, RA, PA), che guardano solo ai vicini diretti. SimRank, invece, ragiona come un'eco che viaggia attraverso la rete: la similarità tra due nodi dipende dalla similarità dei loro vicini, la quale a sua volta dipende dalla similarità dei vicini dei vicini, e così via.

Immagina di voler misurare quanto due nodi u e v assomigliano l’uno all’altro. Secondo SimRank, la domanda fondamentale è:

**“Se parto dai vicini di u e dai vicini di v, quanto è probabile che camminando all’indietro nella rete io arrivi in nodi simili?”**

Non stiamo guardando quanti vicini condividono **ora**, come fanno i Common Neighbors, ma quanto simili sono i loro vicini, e i vicini dei loro vicini, e così via.  
È un processo che si propaga all’indietro nella rete.

Per questo SimRank si dice **global similarity measure**, come Katz o Rooted PageRank.

$$
Sim(u, v) = \frac{c}{|N(u)| |N(v)|} \sum_{x \in N(u)} \sum_{y \in N(v)} Sim(x, y)
$$

- $N(u)$ e $N(v)$ sono i vicini di $u$ e $v$
- $c$ è un fattore di attenuazione $(0<c<1)$
- si parte imponendo $Sim(u,u) = 1$ per ogni nodo

>[!caution] Nota
>La similarità tra $u$ e $v$ è la media attenuata della similarità tra **tutte le coppie** di loro vicini.

Differenza dalle altre heuristics

- **Non è locale**: CN, Jaccard, AA, RA guardano solo ai vicini immediati. SimRank guarda all’intera rete, tramite una propagazione iterativa.
- **Non usa il grado direttamente** come PA.
- **Non usa cammini espliciti** come Katz.
- **Non simula un walker**, diversamente dal Rooted PageRank.
- È basato **unicamente sulla struttura di vicinanza reciproca**, applicata ricorsivamente.
## Differenze tra Local e Global

Le heuristics locali guardano **solo ciò che un nodo può vedere nel suo intorno immediato**.
Un indice locale non sa nulla della forma globale della rete.  
Non vede comunità distanti, non conosce percorsi più lunghi, non può capire se due nodi sono parte della stessa zona profonda del grafo.  
Sa soltanto:
- chi sono i miei vicini
- chi sono i vicini dei miei vicini
- quanti sono, quanto sono grandi, quanto sono simili
Metodi come **Common Neighbors, Jaccard, Adamic–Adar, Resource Allocation, Preferential Attachment** vivono tutti in questa dimensione.  
Sono rapidissimi e molto economici, ma “miopi”: funzionano bene quando la rete è social-like, clusterizzata e relativamente semplice.

Le heuristics globali non si fermano ai vicini immediati, ma considerano:
- quanti percorsi collegano due nodi,
- quanto sono lunghi,
- come si distribuisce un random walker,
- come si propaga la similarità attraverso livelli multipli della rete.
Esempi tipici sono:
- **Katz Index**, che somma tutti i cammini possibili, attenuati
- **SimRank**, che usa la similarità dei vicini e dei vicini dei vicini in modo ricorsivo
- **Rooted (Personalized) PageRank**, che segue un random walker che ritorna sulla radice

















